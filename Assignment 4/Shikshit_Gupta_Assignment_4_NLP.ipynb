{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9U-PnDJswds"
   },
   "source": [
    "# Part III\n",
    "Using the previous two tutorials, please answer the following using an encorder-decoder approach and an LSTM compared approach.\n",
    "\n",
    "Please create a transformer-based classifier for English name classification into male or female.\n",
    "\n",
    "There are several datasets for name for male or female classification. In subseuqent iterations, this could be expanded to included more classifications.\n",
    "\n",
    "Below is the source from NLTK, which only has male and female available but could be used for the purposes of this assignment.\n",
    "\n",
    "```\n",
    "names = nltk.corpus.names\n",
    "names.fileids()\n",
    "['female.txt', 'male.txt']\n",
    "male_names = names.words('male.txt')\n",
    "female_names = names.words('female.txt')\n",
    "[w for w in male_names if w in female_names]\n",
    "['Abbey', 'Abbie', 'Abby', 'Addie', 'Adrian', 'Adrien', 'Ajay', 'Alex', 'Alexis',\n",
    "'Alfie', 'Ali', 'Alix', 'Allie', 'Allyn', 'Andie', 'Andrea', 'Andy', 'Angel',\n",
    "'Angie', 'Ariel', 'Ashley', 'Aubrey', 'Augustine', 'Austin', 'Averil', ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNoT4jGksxM2"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXLgwxB_s5JR"
   },
   "source": [
    "# ENCODER DECODER APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0NVdYvuns3eQ",
    "outputId": "9746e8d4-647f-4d8a-941c-15d144748e80"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/names.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "itwrT70ps_ct"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import names\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Get names from the dataset\n",
    "male_names = names.words('male.txt')\n",
    "female_names = names.words('female.txt')\n",
    "\n",
    "# Create a dataframe with names and their respective labels\n",
    "data = pd.DataFrame({\n",
    "    'name': male_names + female_names,\n",
    "    'gender': ['male'] * len(male_names) + ['female'] * len(female_names)\n",
    "})\n",
    "\n",
    "# Shuffle the dataset\n",
    "data = data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0KIH23tVtQPn"
   },
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Data Preparation\n",
    "from nltk.corpus import names\n",
    "import pandas as pd\n",
    "\n",
    "# Get names from the dataset\n",
    "male_names = names.words(\"male.txt\")\n",
    "female_names = names.words(\"female.txt\")\n",
    "\n",
    "# Create a DataFrame with names and labels\n",
    "data = pd.DataFrame({\n",
    "    \"name\": male_names + female_names,\n",
    "    \"gender\": [\"male\"] * len(male_names) + [\"female\"] * len(female_names),\n",
    "})\n",
    "\n",
    "# Shuffle the dataset\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Split into training and testing datasets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Encode labels (male = 0, female = 1)\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_data[\"gender\"])\n",
    "test_labels = label_encoder.transform(test_data[\"gender\"])\n",
    "\n",
    "# Tokenize the names\n",
    "char_vocab = set(\"\".join(data[\"name\"].str.lower()))\n",
    "char_to_idx = {char: idx + 1 for idx, char in enumerate(sorted(char_vocab))}  # 0 is reserved for padding\n",
    "max_len = max(data[\"name\"].str.len())\n",
    "\n",
    "# Convert names to sequences of indices\n",
    "def name_to_indices(name, char_to_idx, max_len):\n",
    "    indices = [char_to_idx[char] for char in name.lower()]\n",
    "    if len(indices) < max_len:\n",
    "        indices += [0] * (max_len - len(indices))  # padding\n",
    "    return indices\n",
    "\n",
    "train_sequences = torch.tensor(\n",
    "    [name_to_indices(name, char_to_idx, max_len) for name in train_data[\"name\"]]\n",
    ")\n",
    "test_sequences = torch.tensor(\n",
    "    [name_to_indices(name, char_to_idx, max_len) for name in test_data[\"name\"]]\n",
    ")\n",
    "\n",
    "# Define DataLoader for training and testing\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(\n",
    "    list(zip(train_sequences, train_labels)), batch_size=batch_size, shuffle=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    list(zip(test_sequences, test_labels)), batch_size=batch_size, shuffle=False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "rjxvrw_G2Acj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the custom Multihead Attention module\n",
    "class CustomMultiheadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(CustomMultiheadAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        # Learnable parameters for the query, key, and value\n",
    "        self.query_weight = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key_weight = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value_weight = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        # Output projection after attention\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        # Linear transformations\n",
    "        query = self.query_weight(query)\n",
    "        key = self.key_weight(key)\n",
    "        value = self.value_weight(value)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        attention_scores = torch.matmul(query, key.transpose(-2, -1)) * self.scale\n",
    "        attention_weights = nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # Weighted sum\n",
    "        attention_output = torch.matmul(attention_weights, value)\n",
    "\n",
    "        # Final output projection\n",
    "        attention_output = self.out_proj(attention_output)\n",
    "\n",
    "        return attention_output\n",
    "\n",
    "# Custom Encoder Module\n",
    "class CustomEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, hidden_dim, num_layers):\n",
    "        super(CustomEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.attention_layers = nn.ModuleList(\n",
    "            [CustomMultiheadAttention(embed_dim, num_heads) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        # Feedforward layers after each attention layer\n",
    "        self.linear1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, embed_dim)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embedding(inputs)\n",
    "        for attention_layer in self.attention_layers:\n",
    "            attended = attention_layer(embedded, embedded, embedded)\n",
    "\n",
    "            # Apply the feedforward network\n",
    "            ff = self.activation(self.linear1(attended))\n",
    "            embedded = self.linear2(ff) + attended  # Residual connection\n",
    "\n",
    "        return embedded\n",
    "\n",
    "# Custom Decoder Module\n",
    "class CustomDecoder(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, num_layers, num_heads):\n",
    "        super(CustomDecoder, self).__init__()\n",
    "        self.attention_layers = nn.ModuleList(\n",
    "            [CustomMultiheadAttention(embed_dim, num_heads) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        # Feedforward layers after each attention layer\n",
    "        self.linear1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, embed_dim)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, encoded):\n",
    "        decoded = encoded\n",
    "        for attention_layer in self.attention_layers:\n",
    "            attended = attention_layer(decoded, decoded, decoded)\n",
    "\n",
    "            # Apply the feedforward network\n",
    "            ff = self.activation(self.linear1(attended))\n",
    "            decoded = self.linear2(ff) + attended  # Residual connection\n",
    "\n",
    "        return decoded\n",
    "\n",
    "# Transformer-based model for classification with encoder and decoder\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, hidden_dim, num_layers, num_classes):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = CustomEncoder(vocab_size, embed_dim, num_heads, hidden_dim, num_layers)\n",
    "        self.decoder = CustomDecoder(embed_dim, hidden_dim, num_layers, num_heads)\n",
    "        self.fc_layer = nn.Linear(embed_dim, num_classes)  # Classification head\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        encoded = self.encoder(inputs)\n",
    "        decoded = self.decoder(encoded)\n",
    "        pooled_output = decoded.mean(dim=1)  # Global average pooling\n",
    "        logits = self.fc_layer(pooled_output)  # Classification head\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uMeOJRLm2OSR",
    "outputId": "4ed6234a-5dc0-49eb-f0de-911b09915faf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.6616339980058334\n",
      "Epoch 2/10, Loss: 0.6783966583822241\n",
      "Epoch 3/10, Loss: 0.6674214619487974\n",
      "Epoch 4/10, Loss: 0.6651779601921388\n",
      "Epoch 5/10, Loss: 0.66380307617499\n",
      "Epoch 6/10, Loss: 0.6699749467959956\n",
      "Epoch 7/10, Loss: 0.6727512373996141\n",
      "Epoch 8/10, Loss: 0.6679438891722329\n",
      "Epoch 9/10, Loss: 0.8821982647006835\n",
      "Epoch 10/10, Loss: 0.6583422143854688\n",
      "Test Loss: 0.6562270057201386, Test Accuracy: 63.00%\n"
     ]
    }
   ],
   "source": [
    "# Set up the necessary imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Instantiate the transformer model\n",
    "# Define the necessary parameters\n",
    "vocab_size = len(char_to_idx) + 1  # Include padding\n",
    "embedding_dim = 128  # Change based on your model's embedding dimension\n",
    "num_heads = 8  # Number of attention heads\n",
    "hidden_dim = 256  # Hidden dimension for the feedforward network\n",
    "num_layers = 3  # Number of layers in the encoder and decoder\n",
    "num_classes = 2  # Male and female classification\n",
    "\n",
    "# Instantiate the transformer model\n",
    "model = Transformer(vocab_size, embedding_dim, num_heads, hidden_dim, num_layers, num_classes)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # Iterate over the training data\n",
    "    for inputs, labels in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss for monitoring\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss}\")\n",
    "\n",
    "# Testing and evaluation\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Iterate over the test data\n",
    "    for inputs, labels in test_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        test_loss += loss_fn(outputs, labels).item()\n",
    "\n",
    "        # Get predictions\n",
    "        preds = outputs.argmax(dim=1)\n",
    "\n",
    "        # Compute accuracy\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total_samples += len(labels)\n",
    "\n",
    "# Compute test loss and accuracy\n",
    "test_loss /= len(test_loader)\n",
    "accuracy = correct / total_samples\n",
    "\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7-bhn5M2_hq"
   },
   "source": [
    "# LSTM APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yq1hnIa4yzGN",
    "outputId": "50c16fa5-4d5e-4fb9-9f09-c285a03dd3d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.5390259247628888\n",
      "Epoch 2/10, Loss: 0.432989945438639\n",
      "Epoch 3/10, Loss: 0.4003668297175786\n",
      "Epoch 4/10, Loss: 0.3745928277172635\n",
      "Epoch 5/10, Loss: 0.35526387427170675\n",
      "Epoch 6/10, Loss: 0.3357337306267652\n",
      "Epoch 7/10, Loss: 0.3135626289862484\n",
      "Epoch 8/10, Loss: 0.29492099036523445\n",
      "Epoch 9/10, Loss: 0.27399233155813646\n",
      "Epoch 10/10, Loss: 0.2552909168166731\n",
      "Test Loss: 0.4179069498181343, Test Accuracy: 82.32%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from nltk.corpus import names\n",
    "\n",
    "# Data preparation\n",
    "# Get names from the dataset\n",
    "male_names = names.words(\"male.txt\")\n",
    "female_names = names.words(\"female.txt\")\n",
    "\n",
    "# Create a dataframe with names and their respective labels\n",
    "data = pd.DataFrame({\n",
    "    \"name\": male_names + female_names,\n",
    "    \"gender\": [\"male\"] * len(male_names) + [\"female\"] * len(female_names),\n",
    "})\n",
    "\n",
    "# Shuffle the dataset\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "data[\"gender_encoded\"] = label_encoder.fit_transform(data[\"gender\"])\n",
    "\n",
    "# Tokenize the names into character indices\n",
    "char_vocab = set(\"\".join(data[\"name\"].str.lower()))\n",
    "char_to_idx = {char: idx + 1 for idx, char in enumerate(sorted(char_vocab))}  # 0 is reserved for padding\n",
    "max_len = max(data[\"name\"].str.len())\n",
    "\n",
    "# Convert names to sequences of indices with padding\n",
    "def name_to_indices(name, char_to_idx, max_len):\n",
    "    indices = [char_to_idx[char] for char in name.lower()]\n",
    "    if len(indices) < max_len:\n",
    "        indices += [0] * (max_len - len(indices))  # padding\n",
    "    return indices\n",
    "\n",
    "data[\"name_indices\"] = data[\"name\"].apply(lambda x: name_to_indices(x, char_to_idx, max_len))\n",
    "\n",
    "# Split into training and testing datasets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Define a custom Dataset class\n",
    "class NameDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.names = torch.tensor(list(data[\"name_indices\"].values.tolist()))\n",
    "        self.labels = torch.tensor(data[\"gender_encoded\"].values)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.names[idx], self.labels[idx]\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(NameDataset(train_data), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(NameDataset(test_data), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the LSTM classifier\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, num_classes):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embed the input sequences\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Pass through the LSTM layer\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "\n",
    "        # Take the output from the last timestep\n",
    "        output = lstm_out[:, -1, :]\n",
    "\n",
    "        # Pass through the fully connected layer\n",
    "        logits = self.fc(output)\n",
    "\n",
    "        return logits\n",
    "\n",
    "# Instantiate the model\n",
    "embedding_dim = 64  # Set the embedding dimension\n",
    "hidden_dim = 128  # LSTM hidden dimension\n",
    "num_layers = 2  # Number of LSTM layers\n",
    "num_classes = 2  # Male and female classification\n",
    "\n",
    "vocab_size = len(char_to_idx) + 1  # Include padding\n",
    "model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, num_layers, num_classes)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = loss_fn(outputs, labels)  # Compute loss\n",
    "        loss.backward()  # Backward propagation\n",
    "        optimizer.step()  # Update parameters\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss}\")\n",
    "\n",
    "# Test and evaluate the model\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        test_loss += loss_fn(outputs, labels).item()\n",
    "\n",
    "        # Get predictions and calculate accuracy\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total_samples += len(labels)\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "accuracy = correct / total_samples\n",
    "\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "glxIMVbu5yao"
   },
   "source": [
    "# LSTM has better results and accuracy than Encoder Decoder Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NkJLEMbG3bok"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
